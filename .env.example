OPENAI_API_KEY=<not needed when using VicunaLLM>
CHAT_API_URL=http://localhost:5000/api/v1/generate
MEMORIES_PATH=memories/
UPLOAD_PATH=uploads/
DOCUMENT_STORE_NAME=brainchulo_docs
CONVERSATION_STORE_NAME=brainchulo_convos
DATABASE_URL=sqlite:///data/brainchulo.db
ANDROMEDA_URL=http://0.0.0.0:9000/
MODEL_PATH=/models/vicuna-AlekseyKorshuk-7B-GPTQ-4bit-128g
USE_FLOW_AGENTS=true
# CPP Model Example:
GENERAL_MODEL_PATH=/models/open-llama-7B-open-instruct.ggmlv3.q4_0.bin
GENERAL_TOKENIZER_PATH=/models/VMware_open-llama-7b-open-instruct
GENERAL_LOADING_METHOD=CPP

# GPTQ Model Example:
# GENERAL_MODEL_PATH=/models/vicuna-7B-1.1-GPTQ-4bit-128g
# GENERAL_LOADING_METHOD=GPTQ

# HF Model Example
# GENERAL_MODEL_PATH=/models/VMware_open-llama-7b-open-instruct
# GENERAL_LOADING_METHOD=HUGGING_FACE

# Guidance Settings
GUIDANCE_AFTER_ROLE="|>"
GUIDANCE_BEFORE_ROLE="<|"

# Tokenizer Settings
TK_BOOL_USE_FAST=false

# HuggingFace
HF_BOOL_USE_8_BIT=true
HF_BOOL_USE_4_BIT=false
HF_DEVICE_MAP=auto

# GPTQ
GPTQ_INT_WBITS=4
GPTQ_INT_GROUP_SIZE=128
# How many layers loaded into GPU, decrease to save more VRAM at the expense of inference speed
GPTQ_INT_PRE_LOADED_LAYERS=20
GPTQ_DEVICE="cuda"
GPTQ_BOOL_CPU_OFFLOADING=true

# LLaMA CPP
CPP_INT_N_GPU_LAYERS=300
CPP_INT_N_THREADS=12
CPP_BOOL_CACHING=false